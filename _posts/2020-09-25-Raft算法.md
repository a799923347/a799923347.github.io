---
layout:     post
title:      Raft算法
subtitle:   
date:       2020-09-25
author:     Bowen
header-img: img/consensus-blockchain.png
catalog:    true
tags:
    - 共识算法
    - Raft
---

### 介绍
Raft是一个管理副本日志的共识算法，得到和Paxos等价的结果，和Paxos一样高效，但是其结构不同于Paxos。和Paxos相比，Raft更加易于理解，因此为构建一个实际的系统提供了更好的基础。
Paxos难以理解，为了支撑实际系统Paxos的架构需要进行比较复杂的改造，因此无论是需要将算法付诸实践又或是学习Paxos算法的人，过程都比较挣扎。于是作者在反复挣扎后提出了一个新的共识算法，为构造实际系统提供了一个更好的基础。
新算法的主要目标就是可理解性，为现实系统定义一个共识算法，并以一个更加易懂的方式来描述。包括对算法进行结构以及减少状态空间--将算法分解成leader选举、日志副本以及安全性，相对Paxos，减少不确定性以及服务器之间可能产生不一致的场景。
Raft和现存的共识算法在许多方面有相似之处，尤其是概念上和VSR算法非常像，但有几点不同：
* 强leader：Raft使用一个相比其他共识算法更强势的leader管理方式，例如log entries只能从leader流向其他服务器，这简化了日志副本的管理，使得Raft更易懂。
* leader选举：Raft使用随机计时器来选举leader，心跳机制是所有共识算法都有的机制，在心跳机制中加入随机计时器只需要很小的改动，然而就使得能够简单快速地解决节点间的冲突。
* 成员变更：使用一种新的联合共识方法调整集群中的服务器，这使得集群在配置变更期间依然能够持续提供服务。

###  状态机复制
状态机复制用来解决很多分布式系统的容错问题，例如，单主节点的大型系统，GFS、HDFS等都属于这类系统，这类系统都是典型的使用独立的状态机来管理leader选举、存储配置信息，即使leader崩溃也不会对状态机产生影响。Chubby和ZooKeeper是副本状态机的经典案例。
![](https://ftp.bmp.ovh/imgs/2020/09/59ac614410b25009.png)
状态机复制通过日志副本实现，如图1所示，每个服务器都存储了一个包含一系列命令的日志，状态机按顺序执行日志中的命令。每个日志中都包含顺序一致的命令，所以每个状态机都是处理的相同序列的命令。因此所有的状态机是确定性的，每个状态机都能得到相同的状态，保持相同的输出序列。
共识算法的任务则是保持日志副本的一致性。服务器接收来自客户端的命令，并将其添加到日志中，再与其他服务器进行通信以保证即使在部分服务器出现问题的情况下最终也能使每个日志都以相同的顺序保存请求数据。一旦命令复制完成，每个服务器上的状态机则按照日志的顺序处理命令，将结果返回给客户端。因此服务器表现的像是一个单一的、高可靠的状态机。
为现实系统服务的共识算法通常具有如下几个特性：
* 在不考虑拜占庭问题的情况下，即使发生网络延迟、网络分区、丢包、数据包冗余、数据包重排序的情况也能保证安全性。
* 只要多数服务器可以正常操作，服务器之间以及服务器和客户端之间可以互相通信，算法就能提供完整的服务。
* 不依赖计时器来保证日志的一致性：错误的时钟和极端的消息延迟会引发可用性问题。
* 通常情况下，一轮rpc调用中得到多数节点的响应后一个命令就可以算作执行完成，少数较慢的服务器不影响整体性能。

### Paxos的问题
Lamport的Paxos算法几乎是公式问题的同义词：各种教程中都使用Paxos算法进行教学，大多数的共识系统的实现都是从Paxos开始。Paxos首先在单决策的基础上定义了一个用于达成一致的协议，比如单日志记录复制，称其为single-decree Paxos。然后将这个协议下的多个实例组合起来得到一系列决策（multi-Paxos）。Paxos同时保证了安全性和活性，支持集群成员的变更，其正确性已经得到证明，正常情况下效率也很高。
但Paxos有几个明显的短板：
* 异常难以理解。在原本论文(The part-time parliament)中对Paxos的算法的描述不够清晰，即使花费了很大力气也很少有人能弄明白。一些人尝试以一种更简单的方式来解释Paxos，这些解释聚焦在single-decree Paxos，然而还是不好理解。Raft作者在2012年的一次非正式会议上发现，很少有人能和Paxos很舒服的相处，即使是一些经验丰富的研究者也是如此。假设Paxos的晦涩来自单决策子集的选择，single-Paxos被分解成两个步骤，两个步骤并没有简单明了的解释，很难被理解，因此也很难理解为什么单决策协议能起作用，multi-Paxos的构成规则有增加了额外的复杂度。Raft的作者认为多决策的共识问题能够以更加直接易懂的方式被分解掉。
* 第二个问题则是Paxos没有为构造现实实现提供一个好基础。首先multi-Paxos并没有一个广泛达成一致的标准算法，Lamport的描述更多是针对single-decree Paxos，其描绘了可能实现multi-Paxos的几个方法，但细节并没有过多赘述。有一些对Paxos的具体实现和优化的尝试，但这些方法各不相同，且与Lamport的原本描述也有所出入。像Chubby这一类的系统实现了一个类Paxos的算法，但大多数案例中并没有公开实现细节。
* Paxos的架构不适宜用来构建一个现实系统，比如选择日志记录的集合将其合并到一个连续的日志中并没有带来多少好处，反而增加了实现的复杂度。以固定顺序将新日志记录连续追加到日志尾部是一个更简单高效的方案。另一个问题是Paxos以对等网络为核心，这在解决只需要获得一个决策的简单问题时是合理的，但现实系统很少这么做。如果需要作出一系列决策，更简单且快的方法是先选出一个leader，然后由leader来协调决策。

### Raft算法
#### 为可理解性而设计
Raft的设计目标就是为系统构建提供完整、实用的基础，减少开发者在设计系统时的工作量；在所有条件下都能保证安全，典型使用场景下保证可用性，常规操作保证效率。但最重要的也是最难的目标，就是可理解性。算法要能被大众理解，提供直觉上使得算法获得进化的可能性，以便于系统构造者在现实实现不可避免的情况下，对其进行扩展。
可理解性是带有高度主观性的概念，尽管如此，Raft作者从两个方面对算法的可理解性进行衡量：
* 问题分解：将任何可分解的问题进行分解，分解为可单独解决、解释和理解的模块，例如将共识问题分解为leader选举、日志复制、安全性和成员变更。
* 简化状态空间：通过减少需要考虑的状态简化状态空间，是系统更加清晰，减少不确定性。尽管大多数场景下尝试了去减少不确定性，然而在个别场景下不确定性反而能提高可理解性。特别是，随机方法引入了不确定性，但是却能通过以相同的方式处理可能的选举来简化状态空间。例如，使用随机方法简化leader的选举。

#### 算法内容
Raft算法实现共识的方式首先是选举出一个独一的leader，由该leader来管理日志副本。leader接收来自客户端的日志记录，然后将其复制到其他服务器上，再在合适的时机通知各服务器将日志记录应用到状态机上。leader的存在简化了日志副本的管理，leader能够决定存放新的日志记录的地方而不用与其他机器协商，数据从leader流向其他服务器。当leader失败或者与其他服务器断开连接时，则会选举产生新的leader。
根据给定的leader方法，Raft将共识问题分解为三个相对独立的子问题：
* leader选举：当已存在的leader失败时，必须选举产生一个新的leader。
* 日志副本：leader必须接受来自客户端的日志记录，并将其复制到集群中的其他服务器上，强制其他服务器上的日志与自己保持一致。
* 安全性：任何一个服务器将一个日志记录应用到它的状态机上后，则其他服务器在相同的日志索引位置不能应用不同的命令。

具有代表性的Raft集群有五个服务器组成，这样的集群则允许两台机器失效。任一时刻，每台服务器为以下三种状态之一：leader、follower、candidate。正常情况下，只会存在一个leader，其他的服务器都是follower。follower是被动的：follower本身不发出任何请求，只对来自leader和candidate做出响应。leader处理所有来自客户端的请求，如果客户端将请求发送到了follower上，follower则会将请求重定向到leader上。candidate状态用于leader选举，下图展示了各个状态及状态间的转换。
![](https://ftp.bmp.ovh/imgs/2020/09/97dce905156320dc.png)
Raft将时间切分成任意长度的term，如下图所示，term被编号为连续的数字。每个term开始于一次选举，此时candidate会尝试着变成leader。如果某个candidate赢得了选举，在该term的其余时间其将扮演leader角色提供服务。某些情况下选举会导致一次分裂的投票，此时该term则结束，该term任期内没有任何leader产生，集群会迅速开始下一轮选举。Raft保证一个term任期内最多只会有一个leader。
![](https://ftp.bmp.ovh/imgs/2020/09/7846118dcebb8121.png)
不同的服务器在不同的时间可能会观察到term之间的变化，而在某些情况下有的服务器则可能观察不到一次选举甚至是整个term周期。在Raft中，term扮演着逻辑时钟的角色，用于探测过期信息，比如过期的leader。每个服务器保存了当前term的编号，term编号随着时间单调递增，服务器的当前term小于其他服务器时，则会将当前term更新成其他服务器较大的term值；如果candidate或者leader发现自己的term过期了，则会立即将自己的状态恢复成follower。如果服务器收到了一个带有过期term编号的请求，则会拒绝掉这个这个请求。
Raft服务器之间通信使用rpc，共识算法只需要两种类型的rpc，投票rpc--由candidate在选举过程中产生，追加日志rpc--由leader产生并用于复制日志和提供心跳功能。服务器如果没有收到响应会及时的重试rpc，出于性能考虑，rpc会以并行的方式发出。

#### leader选举
Raft使用心跳机制触发leader选举。当服务器启动后，一开始的角色都是follower，只要服务器一直能够收到来自leader或者candidate的验证rpc，服务器就会一直保持住follower的状态。leader发送周期性的心跳(不带日志记录数据的AppendEntries RPC)给所有的follower来保持leader的权限，如果在选举超时时间内follower没有收到leader的心跳，follower会假设没有可用的leader并开始新一轮选举。
为了发起一次选举，follower会增加他的term值并将自己的状态转换成candidate。然后给自己投票，向集群中的其他服务器发布RequestVote RPC。candidate会一直持续这个状态，直到满足下列任一条件：
* 该服务器赢得了选举；
* 另一个服务器成为了leader；
* 一段时间后没有任何服务器赢得选举；

在同一个term中，如果某个candidate收到了整个集群中多数服务器的投票，则该candidate赢得本次选举。每个服务器在一个term周期内最多只能给一个candidate投票，投票规则为先到先得，即将票投给同一个term周期内最先收到的请求。多数规则保证了一个term周期内最多只会有一个candidate赢得选举，一旦某个candidate赢得了选举，该candidate就会成为leader。然后发送心跳给所有其他的服务器建立权力防止新的选举。
在等待投票结果的过程中，candidate可能会受到来自其他服务器声明成为leader的AppendEntries RPC，如果leader的term不小于该candidate的当前term，则该candidate会承认leader的合法性并转变为follower。如果RPC中的term小于该candidate当前的term，则会拒绝响应RPC，继续保持candidate状态。
第三种可能的结果就是没有任何candidate赢得选举。如果有很多follower同时成为candidate，投票则会出现分歧以至于没有candidate能获得多数选票。发生这种情况时，candidate会超时并增加term值发起新一轮选举。然而如果没有额外的处理，有分歧的投票可能会无限重复下去。
Raft使用随机的选举超时来保证有分歧的投票只是极少数的情况并且发生时能够被迅速解决。为了在一开始就避免投票分歧的情况，选举超时时间从一个固定的时间间隔(150-300ms)获得。这使得服务器在超时时间上分布足够分散，所以大多数情况下只有一个服务器会超时，该服务器会赢得选举并在其他服务器超时之前向其发送心跳消息。同样的机制被用于处理有分歧的投票。在开始一次选举时每个candidate重置它的随机选举超时时间，超时后则开始新一轮选举，这种方法减少了选举产生分歧的可能性。

#### 日志副本
当leader被选出出来之后，就开始响应客户端的请求。每个客户端请求都包含了需要被副本状态机执行的command，leader将command作为一个新的记录追加到日志尾部，然后通过向集群中的其他机器发布AppendEntries RPC复制记录。当记录被安全复制后，leader才会将该记录应用到自己的状态机上，并将执行结果返回给客户端。如果follower崩溃或者运行缓慢，又或者网络数据包丢失，leader会一直重试发布AppendEntries RPC，直到所有的follower都完成对日志记录的存储。
log的格式如下图所示，记录同时包含了一个状态机命令和term值，日志记录中的term值用于探测日志间的不一致性以及保证以下几个特性：
* 选举安全性：一个给定term中最多只会产生一个leader。
* leader只追加日志：leader不会覆盖或删除日志中的记录，之后再日志尾部追加新记录。
* 日志匹配：如果两个日志包含一个index和term都相同的记录，则两个日志中在该索引之前的记录也都保持一致。
* leader完整性：如果在某个term内一个日志记录已经被提交，那么在所有拥有更高term的leader日志中必定会存在该日志记录。

![](https://ftp.bmp.ovh/imgs/2020/09/b047e5540c2edfb1.png)

每个日志记录都有一个整数索引用于标记其在日志中的位置。leader将决定何时能安全地将日志记录应用到状态机上，这个操作称作提交。Raft保证已提交的记录是持久化的，并最终会被其他可用的状态机执行。一旦多数服务器完成记录的复制，leader则会提交自己的日志记录，提交操作还会触发提交leader日志中该日志记录之前的记录，包括之前历任leader创建的记录。leader会持续跟踪需要提交的最新索引位置，包括发布的AppendEntries RPC中的索引以便于其他服务器能知道各自需要提交的索引位置。当follower发现一个日志记录被提交后，follower则会将该记录应用到本地的状态机上。

Raft作者在设计Raft日志机制时保持了不同服务器日志间高度的连续性。这不仅简化了系统的行为，使得系统的行为更具有可预测性，也是保证安全性的重要组件。Raft保持了如下几个性质，一起构成了`日志匹配`的特性：
* 如果不同日志中的两个记录的index和term值相同，则记录中的命令也相同。
* 如果不同日志中的两个记录的index和term值相同，则两个日志在该记录之前的所有记录都相同。

第一个性质基于这样的事实：对于给定的term和index，leader最多只会创建一个记录，而且已经创建后记录在日志中的位置不会再发生变化。
第二个性质则由AppendEntries执行的一个简单的一致性检查来保证。当发送一个AppendEntries RPC时，leader会在请求中带上前一个日志记录的index和term，如果follower在本地的日志中找不到相同index和term的记录，follower会拒绝此次请求。这个一致性检查方法类似`数学归纳法`：初始化的空状态一定满足`日志匹配`，在任意时刻扩展日志时，一致性检查都要求保持日志匹配特性。所以，只要AppendEntries返回成功，leader就可以认为follower的日志和自己保持一致。
正常操作过程中，leader和follower的日志保持一致，所以AppendEntries的一致性检查不会失败。但当leader崩溃时就会产生不一致，旧leader可能还没来得及将所有记录复制到所有follower上。随着一系列的leader和follower的崩溃，不一致的情况会加剧。下图阐述了follower的日志可能与leader不一致的几种情况，follower相比leader上的记录可能会有缺失，也可能比leader多，或者两种情况都有。这种情况可能会持续经历多个term周期。
![QQ截图20200923222709.png](https://i.loli.net/2020/09/23/ATtVXEOfSzcpqob.png)
Raft算法中，leader通过强制follower复制自己的日志来解决不一致。这意味着follower中有冲突的日志会被来自leader的日志覆盖。为了使follower的日志变成和自己一致，leader需要找到两个日志开始产生不一致的位置，删除follower日志中该位置后的所有记录，然后将leader中该位置后的记录发送给follower。所有这些操作均发生在响应AppendEntries RPC执行一致性检查的时候。leader为每个follower维护一个`nextIndex`，指向下一个leader即将发送给follower的索引位置。当leader第一次上台后，会将`nextIndex`初始化为自己日志的最后一个索引的下一位（上图中索引值11）。如果follower的日志和leader的不一致，下一次AppendEntries RPC的一致性检查则会失败。请求被拒后，leader会减低`nextIndex`的值并重试AppendEntries RPC。最终`nextIndex`会指向leader和follower能匹配上的地方，此时一致性检查就能通过了，follower上有冲突的记录会被删除，并将leader上的记录追加到follower上。一旦AppendEntries成功后，在该term剩余时间内follower和leader会一直保持一致。

### 安全性
在leader提交日志记录时，follower可能不可用，而如果此时该follower又被选举为leader，那些新写入的日志记录可能会被覆盖，因此不同的状态机则可能执行着不同的command序列。Raft算法在leader的选举上添加了一些限制，这些限制保证了任何一个term的leader都包含了所有历史term已提交的记录。

#### 选举约束
对于任意基于主节点的共识算法，leader最终都必须持有所有已提交的日志记录。例如在VSR算法中，被选举出来的leader初始时可以不用包含所有的已提交的记录，在选举过程中或者选举结束不久后，算法有额外的机制来补齐丢失的记录并将其发送到新leader上。然而这导致了额外的无法令人忽视的复杂度。Raft使用了一个更简单的方法：在选举时，保证新leader包含了历史term任期所有已提交的记录，而不是从其他地方将这些记录传输到leader节点上。这意味着日志记录只会单一的从leader流向follower，而leader绝对不会覆盖本地log中已存在的记录。
Raft使用投票处理来避免未包含完整日志提交记录的节点被选举为leader。candidate要想被选举为leader必须与集群中的多数节点进行通讯，这意味着这些节点中至少有一个节点包含了最新的日志记录。如果一个candidate与这样的`多数`节点上的日志保持同步或者更新，则此candidate则持有了所有已提交的记录。Raft中该功能通过`RqquestVote RPC`来实现：RPC中包含了candidate的日志信息，如果本身的日志比candidate要新，那么投票者则会拒绝为该candidate投票。
Raft通过比较日志最后一个记录的`index`和`term`来确定两个日志哪个是最新的，如果`term`不同，则`term`较大的日志是最新的，如果`term`相同，则认为`index`较大的为最新。

#### 提交往期term的记录
leader在记录被复制到多数节点后会进行提交，如果在提交之前leader崩溃了，继任leader则会尝试完成对记录的复制工作。但是，即使日志记录已经被复制到了多数节点上，leader也无法立即得到该记录已经被提交了的结论。下图对这种场景进行了说明，即使日志记录被复制到了多数节点上，仍有可能被继任的leader覆盖掉。
![WX20200925-133415@2x.png](https://i.loli.net/2020/09/25/fbaUKGo7ky1LQPj.png)
为了解决这个问题，Raft算法不会通过统计副本的数量来提交往期term的日志记录，只有日志记录来自leader的当前任期才会被提交。通过这种方式，当来自当前任期的日志被提交时，由于`日志匹配`的特性，之前往期的记录也会间接地被提交掉。某些情况下leader也可以安全的认定旧的日志记录已经被提交，比如日志已经被复制到了所有的节点上，而Raft为了简单采取了更加保守的方法。
当leader复制往期term的记录时，由于日志记录保持了其原始的term编号，Raft在提交规则上引入了这个额外的复杂性。在其他共识算法中，当新leader复制来自往期term的记录时会给予记录一个新的term编号。Raft通过保持term不变使这个过程变得更加简单，而且，相比其他算法，Raft算法中新的leader被选举出来后，当需要向其他服务器发送往期记录的数据时，可以发送更少的数据就能满足使用，其他算法则需要全量发送冗余日志记录来对往期term的日志重新编号。

### follower和candidate崩溃
相比leader，处理follower和candidate的崩溃要简单得多，且两者的处理方式是一样的。当follower或者candidate崩溃时，接下来向其发送`RequestVote`和`AppendEntries RPC`则会失败，Raft的应对方式就是无限重试，如果崩溃的机器重新上线，那么RPC就会成功。如果服务器在完成RPC处理但在返回响应之前崩溃了，重启后会继续受到相同的RPC，Raft的RPC是幂等的，所以重复调用不会产生负面效果。

### 时间设置和可用性
Raft算法的要求之一就是安全性不依赖于时间设置：系统不会应为一些事件发生的快慢而产生错误的结果。然而可用性却不得不依赖于时间设置。比如，通信时间超出了通常服务器的崩溃间隔时间，那么candidate将无法获得足够的时间来赢得选举；没有一个稳定的leader，Raft无法提供服务。
leader选举时Raft中时间设置要求最严格的地方。当满足下列时间设置要求时，Raft才能进行选举并尽可能长的维持一个稳定的leader：
```
broadcastTime <= electionTimeout <= MTBF
```
上述不等式中，`broadcastTime`是服务器之间RPC消耗的平均时间，`electionTimeout`是选举超时时间，`MTBF`服务器两次失败请求的平均间隔时间。广播时间不能大于选举超时时间，以便于leader能稳定地发送所需的心跳信息以保证和follower之间的联系。使用了随机的选举超时时间后，这个不等式则排除了产生分裂投票的可能性。选举超时时间不能超过MTBF时间，以便于系统提供稳定的服务。因为如果选举超时时间超过了MTBF，将很难触发下一次选举。当leader崩溃后，系统在选举超时时间内会处于短暂的不可用状态，然而这相对整体时间来讲占比非常小。
广播时间和MTBF是系统本身的属性，而选举超时时间则可以由我们来设置。Raft的RPC通常接收方需要将数据持久化，所以广播时间在0.5ms到20ms之间，取决于存储技术。所以，选举超时时间大概在10ms到500ms之间。通常服务器MTBF能达到几个月甚至更长，满足MTBF条件很容易。

### 成员变更
本文内容到目前为止都是建立在集群配置没有发生变更的假设上，在实际使用中，偶尔变更集群配置是必要的，比如替换发生故障的机器或者修改日志副本数量要求。尽管可以通过下线这个集群，更新配置文件再重新上线集群，但这会使集群在整个操作过程中都出于不可用的状态，而且人为进行操作无形中也增加了出错的可能性。为了避免这类情况，Raft集成了自动配置变更的能力。
为了保证配置变更机制的安全性，在过渡时期必须杜绝一个term内在某个时间点上产生两个leader的可能性。不幸的是，任何将就配置直接切换成新配置的方法都是不安全的，不可能同时自动完成所有服务器的配置切换，所以在过渡期可以将集群切分成两个独立的`多数`。如下图所示：
![QQ截图20200926231425.png](https://i.loli.net/2020/09/26/D2BdklZUjY4XcV1.png)
为了保证安全性，配置变更使用两阶段方法，例如，一阶段下线就的配置，这样系统就无法处理客户端请求，然后二阶段启用新配置。Raft算法中首先切换成一个称作`联合共识`的过渡配置，一旦联合共识提交后，系统就能过渡到新配置上。联合共识将新旧配置合并到一起：
* 日志记录被复制到两个配置中的所有服务器上。
* 两个配置中的服务器都有可能被选举为leader。
* 选举和日志提交需要满足两个配置集群中的多数要求。

联合共识允许节点在不同的时间点从旧配置过渡到新配置上，而不用向安全性妥协，允许集群在配置切换期间继续处理客户端请求。
集群配置的存储和提交使用副本日志中特定的记录，下图阐述了配置变更的过程。当leader接收到将配置从C<sub>old</sub>变更到C<sub>new</sub>时，将配置作为日志记录保存下来，并将该记录按照前面所说的复制策略复制到其他节点上。接收到该记录的节点将配置添加到日志中后，该配置将被用于接下来的决策中(节点使用日志中的最新配置，无论该记录是否已提交)。这意味着leader将使用C<sub>old,new</sub>的规则来决定何时提交相应配置的日志记录。当leader崩溃时，将使用C<sub>old</sub>或者C<sub>old,new</sub>选举出新leader，使用哪种配置由赢得选举的candidate是否已经接收到了C<sub>old,new</sub>配置。这段期间，任何情况下都不能只单方面基于C<sub>new</sub>做出决策。
![QQ截图20200927012305.png](https://i.loli.net/2020/09/27/Uy2NcTCXHDRxZmw.png)
一旦C<sub>old,new</sub>被提交，无论是C<sub>old</sub>还是C<sub>new</sub>没有另一方的批准都不能单独做出决策，leader完整性属性保证了只有配置变更为C<sub>old,new</sub>的节点才能被选举为leader。此时leader就可以生成C<sub>new</sub>的日志记录并将其复制到集群中，同样的，各服务器节点能看到该记录时，则C<sub>new</sub>开始发挥作用。当新配置被提交后，旧配置就不会再产生作用，此时新配置列表以外的节点则可以被关闭。如上图所示，不存在某个时间，C<sub>old</sub>和C<sub>new</sub>都能进行单方面的决策，要么是C<sub>old</sub>结合C<sub>old,new</sub>，要么是C<sub>old,new</sub>结合C<sub>new</sub>。
关于配置更新还有另外三个问题需要提出来讨论。第一个问题就是新节点初始时可能不包含任何日志记录，如果节点以这种状态被添加到集群中，需要花费相当长的一段时间来追赶日志的进度，在此期间，节点将不能提交新的日志记录。为了避免这个问题，Raft在配置变更之前引入了一个附加的阶段，在该阶段，新加入到集群中的节点不需要参与投票，leader仍然会向其发送复制日志，但节点不在`多数`要求的考虑范围之内。一旦新节点追赶上日志复制的进度，则会进行之前所描述的配置变更处理。
第二个问题是leader可能不在新配置的节点列表内。这种情况下，当完成C<sub>new</sub>记录的提交后，leader会自行退出，转换成follower状态。这意味着在某个时间段(提交新配置时)，leader将管理着一个自己并不包括在内的集群，leader复制着日志记录但自己并不在`多数`节点的考虑范围内。leader在C<sub>new</sub>提交时变更，因为这是新配置能独立发挥作用的第一个时间点，在此之前，可能只有C<sub>old</sub>配置中的节点才会被选举为leader。
第三个问题是移除不在配置C<sub>new</sub>中的节点时可能会对集群产生干扰。这些节点将不会再收到leader的心跳，所以会触发超时并开始新的选举。然后这些节点会发送带有新term的`RequestVote RPC`，引发当前leader退回到follower状态。但即使选出了新leader，但被移除的节点仍然会再次超时又再次开始选举，周而复始，导致较差的可用性。
为了避免这个问题，当节点相信当前已经有一个leader存在时，则忽略RequestVote RPC。特别地，当节点在监听当前leader的最小选举超时时间内收到投票请求时，其不会更新term或者授权投票。这不会影响正常的选举，其他节点仍然在等待各自的选举时间超时来触发新选举，然而这避免了因为移除节点导致的系统紊乱：如果leader能够接受到集群的心跳，则不会被更大term的选举替换掉。

### Reference
1. Ongaro D, Ousterhout J. In search of an understandable consensus algorithm[C]//2014 {USENIX} Annual Technical Conference ({USENIX}{ATC} 14). 2014: 305-319.
2. Oki B M, Liskov B H. Viewstamped replication: A new primary copy method to support highly-available distributed systems[C]//Proceedings of the seventh annual ACM Symposium on Principles of distributed computing. 1988: 8-17.
2. https://www.cnblogs.com/xybaby/p/10124083.html
3. 可视化工具：http://thesecretlivesofdata.com/raft/